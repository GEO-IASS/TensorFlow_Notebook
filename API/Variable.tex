\section{tf.Vairable}
\subsection{Variable类}
一个变量通过在图上运行run()方法维持变量状态，你可以构造一个Variable类的实例添加到图上。
Variable()够着要求一个初始值，这个值可以是一个任何类型和形状的Tensor。初始值定义了变量的形状了类型。在构造后变量的形状和类型就被固定，值可以通过assign方法更改。如果你之后像改变变量的形状可以用assign操作设置validate\_shape=False。和任何Tensor一样Variable()创建的变量可能被用于图中的操作的输入。另外所有的操作重载了Tensor类为变量，因此你可以通过在变量上做算法添加节点到图上。
\begin{python}
import tensorflow as tf

# Create a variable.
w = tf.Variable(<initial-value>, name=<optional-name>)

# Use the variable in the graph like any Tensor.
y = tf.matmul(w, ...another variable or tensor...)

# The overloaded operators are available too.
z = tf.sigmoid(w + y)

# Assign a new value to the variable with `assign()` or a related method.
w.assign(w + 1.0)
w.assign_add(1.0)
\end{python}
当你启动图运行操作前变量需要被明确的初始化。你可以通过初始化器操作，从保存的文件恢复或者仅仅运行一个assign操作给变量指定值。事实上，变量初始化操作仅仅是一个赋值给变量初始值的assgin操作。
\begin{python}
# Launch the graph in a session.
with tf.Session() as sess:
    # Run the variable initializer.
    sess.run(w.initializer)
    # ...you now can run ops that use the value of 'w'...
\end{python}
通常初始化是添加global\_vaiables\_initializer()操作到图上初始化所有的变量。你然后启动图后运行Op
\begin{python}
# Add an Op to initialize global variables.
init_op = tf.global_variables_initializer()

# Launch the graph in a session.
with tf.Session() as sess:
    # Run the Op that initializes global variables.
    sess.run(init_op)
    # ...you can now run any Op that uses variable values...
\end{python}
如果你创建一个变量的时候它的初始值依赖于另一个变量，用其它的变量的initialized\_value()。确保变量按照正确的顺序初始化。当它们被创建的时候所有的变量被自动地收集在图中。默认，构造体添加变量到图上手机GraphKeys,GLOBAL\_VARIABLES。一个方便的函数是global\_variables()返回集合里面的内容。

当创建一个机器学习模型的时候区别保持可训练模型参数和其它的变量像用于记录训练步数global step变量 是很方便的。为了使这个更简单，变量构造体支持trainable=<bool>参数，如果为True，新的变量也被添加到图集合GraphKeys.TRAINABLE\_VARIABLE。方便的函数trainable\_variables()返回这个集合的内容，Optimizer类用这个集合作为优化的默认的列表变量。
\begin{center}
\begin{tabular}{|c|c|}
\hline
属性&功能\\
\hline
device&变量所在的设备\\
\hline
dtype&变量的数据类型\\
\hline
graph&变量所在的图\\
\hline
initial\_value&用作变量初始值的tensor\\
\hline
initializer&对于变量的初始化操作\\
\hline
name&变量的名字\\
\hline
op&这个变量的Operation\\
\hline
shape&这个变量的tensorShape\\
\hline
\end{tabular}
\end{center}
\subsection{方法}
\_\_init\_\_
\begin{python}
__init__(
    initial_value=None,
    trainable=True,
    collections=None,
    validate_shape=True,
    caching_device=None,
    name=None,
    variable_def=None,
    dtype=None,
    expected_shape=None,
    import_scope=None
)
\end{python}
创建一个值为initia\_value的新的变量，新的变量被添加到collections的列表中，默认是GraphKeys.GLOBAL\_VARIABLES。如果trainable是True变量被添加到集合GraphKeys,TRAINABLE\_VARIABLE。这个构造体创建一个variable操作和assign操作设置它的初始值。
参数:
\begin{itemize}
\item initia\_value:一个Tensor，或者Python对象转化为一个Tensor，是Variable的初始值。初始值必须有一个指定的形状厨卫validate\_shape被设置为False。没有参数可能被调用，当被调用的时候返回初始值、在这种情况下，dtype必须被指定（注意initi\_ops.py的初始化函数必须在使用前被限制形状）
\item trainable:如果为真(默认)，添加变量到GraphKeys.TRAINABLE\_VARIABLE集合。这个集合Optimizer类用作默认的变量列表集合。
\item collections:集合keys的列表，新的变量被添加到这些集合。默认是GraphKeys.GLOBAL\_VARIABLE。
\item validate\_shape:如果为False允许变量初始化时不指定形状。如果为True，默认。initial\_value必须知道。
\item caching\_device:描述变量应该被缓存的设备的字符串。默认是Variable的设备。如果不是None，cache在另一个设备上。通常操作保存变量时缓存在设备上，通过Switch复制在不同的条件状态下转换。
\item name:变量的名字，默认为'Variable'自动设置为独一无二。
\item variable\_def:VariableDef protocal buffer.如果不为None，用它的值重新创建变量对象，访问在图上变量的节点，节点必须存在。图不美改变，variable\_def和其它的参数被相互排斥。
\item dtype:如果设置，init\_value将被转化为给定的类型。如果设置为None，数据类型将被保持如果初始值是一个Tensor)或者convert\_to\_tensor将转换。
\item expected\_shape:一个TensorShape。如果设置，initial\_value期望的形状。
\item import\_scope:选项字符串，Name scope添加到Variable仅仅在protocol buffer初始化时使用。
\item[ValueError]
\begin{itemize}
	\item ValueError:如果variable\_def和initial\_value被指定。
	\item valueError:如果初始值没有指定或者没有形状同时validate\_shape是True。
\end{itemize}
\end{itemize}
\_\_abs\_\_
计算tensor的绝对值，给定一个复数x，这个操作将返回一个float32或者float64类型的Tensor，它的值是元素的模($\sqrt{a^2+b^2}$)
\begin{python}
# tensor 'x' is [[-2.25 + 4.75j], [-3.25 + 5.75j]]
tf.complex_abs(x) ==> [5.25594902, 6.60492229]
\end{python}
参数:
\begin{itemize}
	\item x:一个数据类型为float32,float64,int32,int64,complex64或者complex128的Tensor或者SparseTensor
	\item name:操作的名字
	\item[Returns]一个Tensor或者相同尺寸和类型的的SparseTensor作为x的绝对值。注意对于complex65或者complex128输入，返回的Tensor将是分别是float32和float64。
\end{itemize}
\_\_add\_\_\newline
\begin{python}
__add__(
    a,
    *args
)
\end{python}
返回按元素相加的结果，Add支持广播运算，AddN不支持。

参数:
\begin{itemize}
	\item x:一个tensor,必须是下面的数据类型：half, float32, float64, uint8, int8, int16, int32, int64, complex64, complex128, string。
	\item y:一个Tensor们必须和x的类型相同。
	\item name:操作的名字
	\item[Returns]:和x相同类型的Tensor。
\end{itemize}
\_\_adn\_\_\newline
\begin{python}
__and__(
    a,
    *args
)
\end{python}
返回xy按位想与的值，注意LogicalAnd支持广播。

参数:
\begin{itemize}
	\item x:一个bool型参数。
	\item y:一个bool型参数。
	\item name:操作的名字。
	\item[Returns]:bool型的返回值。
\end{itemize}
\_\_div\_\_\newline
\begin{python}
__div__(
    a,
    *args
)
\end{python}
用Python2的语法除两个数

参数:
\begin{itemize}
	\item x:数值类型的分子Tensor。
	\item y:数值类型的坟墓Tensor。
	\item name:操作的名字
\end{itemize}
返回:x除以y的商。

\_\_floordiv\_\_\newline
x和y按元素相除截取为接近最小负整数(c语言实现，十分高效)。
x和y必须有相同的类型，结果也有相同的类型。
参数:
\begin{itemize}
	\item x:实数类型的分子tensor
	\item y:实数类型的分母tensor
	\item name:操作的名字
	\item[Returns]x/y截取后的结果
	\item[Raises]:TypeError(如果输入为复数)。
\end{itemize}

\_\_ge\_\_\newline
\begin{python}
__ge__(
    a,
    *args
)
\end{python}
按元素返回x>=y的值。GreaterEqual支持广播。

参数:
\begin{itemize}
	\item x:一个Tensor，必须是loat32, float64, int32, int64, uint8, int16, int8, uint16, half。
	\item y:一个和x类型相同的Tensor。
	\item name:操作的名字。
	\item[name]:bool型的tensor。
\end{itemize}

\_\_getitem\_\_\newline
\begin{python}
__getitem__(
    var,
    slice_spec
)
\end{python}
创建一个slice helper，从当前变量的内容中创建一个子tensor。这个函数被允许赋值给一个IE片的范围。这类似于Python中的\_\_setitem\_\_函数函数。然而语法的不同用户可以捕获复制操作来分组或者喘气sess.run(),例如:
\begin{python}
import tensorflow as tf
A = tf.Variable([[1,2,3], [4,5,6], [7,8,9]], dtype=tf.float32)
with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  print(sess.run(A[:2, :2]))  # => [[1,2], [4,5]]

  op = A[:2,:2].assign(22. * tf.ones((2, 2)))
  print(sess.run(op))  # => [[22, 22, 3], [22, 22, 6], [7,8,9]]
\end{python}
注意复制不支持NumPy广播语法.

参数:
\begin{itemize}
	\item var:op.Variable对象。
	\item slice\_spec:tensor.\_\_getitem\_\_参数
	\item[Returns]:合适的tensor切片，基于slice\_spec,作为一个操作，这个操作也有assign()方法用于生成一个赋值操作.
	\item[Raises]:
	\begin{itemize}
		\item ValueError:如果切片的范围是负的大小。
		\item TypeError：如果切片索引不是整数,或者省略。
	\end{itemize}
\end{itemize}
\_\_gt\_\_\newline
\begin{python}
__gt__(
    a,
    *args
)
\end{python}
返回x>y的bool Tensor Greater支持广播运算。
\begin{itemize}
	\item x:一个Tensor，数据类型为float32, float64, int32, int64, uint8, int16, int8, uint16, half
	\item y:一个Tensor和x相同的数据类型。
	\item 操作的名字
	\item[Returns]:bool型的Tensor。
\end{itemize}

\_\_invet\_\_\newline
\begin{python}
__invert__(
    a,
    *args
)
\end{python}
按元素返回x的非。

参数:
\begin{itemize}
\item x:一个bool型的Tensor。
\item name:操作的名字。
\item[Returns]:bool型的Tensor。
\end{itemize}

\_\_iter\_\_\newline
阻止迭代的伪方法，不要调用。注意我们如果注册getitem作为重载操作，Python将尝试在0到无穷大迭代，声明这个方法阻止无意识的行为。
异常：TypeError：调用的时候。

\_\_le\_\_\newline
\begin{python}
__le__(
    a,
    *args
)
\end{python}
按元素返回x<=y的值，LessEqual支持广播。

\subsection{参数}
\begin{itemize}
	\item x:一个Tensor，必须是下面的数据类型:float32, float64, int32, int64, uint8, int16, int8, uint16, half。
	\item y:一个Tensor，和x的类型一样。
	\item name:操作的名字。
	\item[Returns]:返回bool型的Tensor。
	\end{itemize}

\_\_it\_\_\newline
\begin{python}
__lt__(
    a,
    *args
)
\end{python}
an元素返回x<y的值。Less支持广播运算。

参数:
\begin{itemize}
	\item x:一个Tensor，类型如下:float32, float64, int32, int64, uint8, int16, int8, uint16, half。
	\item y:一个Tensor，必须和x的类型相同。
	\item name:操作的名字。
	\item[Returns]:bool型的Tensor。
\end{itemize}

\_\_matmul\_\_\newline
\begin{python}
__matmul__(
    a,
    *args
)
\end{python}
矩阵乘法a*b,两个矩阵的类型都是float16, float32, float64, int32, complex64, complex128，矩阵可以通过设置flag为True为转置矩阵或者伴随矩阵，默认flag为False。如果两个矩阵包含一些0，为了更高效的乘法设置相应的a\_is\_sparse或者b\_is\_sparse。这里默认为False，优化仅仅对数据类型为bfloat16或者float32的二维tensor。
例如:

\begin{python}
# 2-D tensor `a`
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3]) => [[1. 2. 3.]
                                                      [4. 5. 6.]]
# 2-D tensor `b`
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2]) => [[7. 8.]
                                                         [9. 10.]
                                                         [11. 12.]]
c = tf.matmul(a, b) => [[58 64]
                        [139 154]]

# 3-D tensor `a`
a = tf.constant(np.arange(1, 13, dtype=np.int32),
                shape=[2, 2, 3])                  => [[[ 1.  2.  3.]
                                                       [ 4.  5.  6.]],
                                                      [[ 7.  8.  9.]
                                                       [10. 11. 12.]]]

# 3-D tensor `b`
b = tf.constant(np.arange(13, 25, dtype=np.int32),
                shape=[2, 3, 2])                   => [[[13. 14.]
                                                        [15. 16.]
                                                        [17. 18.]],
                                                       [[19. 20.]
                                                        [21. 22.]
                                                        [23. 24.]]]
c = tf.matmul(a, b) => [[[ 94 100]
                         [229 244]],
                        [[508 532]
                         [697 730]]]

# Since python >= 3.5 the @ operator is supported (see PEP 465).
# In TensorFlow, it simply calls the `tf.matmul()` function, so the
# following lines are equivalent:
d = a @ b @ [[10.], [11.]]
d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])
\end{python}
参数：
\begin{itemize}
	\item a:一个Tensor，数据类型为 float16, float32, float64, int32, complex64, complex128，rank > 1
	\item b:一个和a rank和数据类型相同的Tensor
	\item transpose\_a:如果为True，乘法前先转置。
	\item transpose\_b:如果为True，乘法前先转置。
	\item adjoint\_a:如果为True，乘法前共轭转置。
	\item adjoint\_b:如果为True，乘法前共轭转置。
	\item a\_is\_sparse:如果为True，a被当做稀疏矩阵。
	\item b\_is\_sparse:如果为True，a被当做稀疏矩阵。
	\item name:操作的名字
	\item[Returns]:一个和a，b有相同类型的Tensor，a，b的乘。矩阵乘。
	\item[Raises]:ValueError:如果transpose\_a和adjoint\_a或者transpose\_b和adjoint\_b都设置为True。
	\end{itemize}
\_\_mod\_\_\newline

\begin{python}
__mod__(
    a,
    *args
)
\end{python}
参数：

\begin{itemize}
	\item x:一个Tensor，数据类型为int32, int64, float32, float64。
	\item y:一个Tensor，类型和x相同。
	\item name:操作的名字
	\item[Returns]:和x相同类型的Tensor。
\end{itemize}
\_\_mul\_\_\newline
\begin{python}
__mul__(
    a,
    *args
)
\end{python}
稀疏和非稀疏乘法。
\begin{python}
__neg__(
    a,
    *args
)
\end{python}
计算元素的负值。

参数:
\begin{itemize}
	\item x:一个Tensor，必须是half, float32, float64, int32, int64, complex64, complex128。
	\item name:操作的名字。
	\item[Returns]:和x的类型相同
\end{itemize}
\begin{python}
__or__(
    a,
    *args
)
\end{python}
LogicalOr支持广播运算，输入x：bool Tensor。y:bool:Tensor,name:操作的名字，返回值bool的Tensor。
\begin{python}
__pow__(
    a,
    *args
)
\end{python}
按元素计算$x^y$,x,y:loat32, float64, int32, int64, complex64, or complex128,name:操作的名字。
\begin{python}
__radd__(
    a,
    *args
)
\end{python}
返回x+y按元素相加，支持广播运算，x,y是一个half, float32, float64, uint8, int8, int16, int32, int64, complex64, complex128, string的Tensor，name是操作的名字。
\begin{python}
__rand__(
    a,
    *args
)
\end{python}
x和y按元素像与，LogicalAnd支持广播，x,y是bool型的tensor,name是操作的名字。
\begin{python}
__rdiv__(
    a,
    *args
)
\end{python}
x,y分别是分子分布tensor，name是操作的名字。
\begin{python}
__rfloordiv__(
    a,
    *args
)
\end{python}
x,y分别是分子分母tensor，name是操作的名字。x/y像0或者负整数靠近。
\_\_rmatmul\_\_,\_\_rmod\_\_,\_\_rmul\_\_,\_\_ror\_\_,\_\_rpow\_\_参照上面的matmul，mod,rmul,or,pow。
\begin{python}
__rsub__(
    a,
    *args
)
\end{python}
按元素x-y。\_\_rturediv\_\_,\_\_rxor\_\_(x异或y),\_\_sub\_\_。
\begin{python}
assign(
    value,
    use_locking=False
)
\end{python}
赋值新的value，use\_locking:如果为True在副职期间用locking，返回赋值后的新值。
\begin{python}
assign_add(
    delta,
    use_locking=False
)
\end{python}
添加delta被计算后保持新值。
\begin{python}
assign_sub(
    delta,
    use_locking=False
)
\end{python}
计算减去delta后只用新值。
count\_up\_to(limit):增加这个变量直到达到limit。\newline
eval(sess=None):在一个绘画中计算返回这个变量，它不是一个图机构的方法，不增加操作到图上。这是一个方面的方法当启动的图上包含有变量的时候，如果没有session传入，用默认的会话。
\begin{python}
assign_sub(
    delta,
    use_locking=False
)
\end{python}
\begin{python}
from_proto(
    variable_def,
    import_scope=None
)
Return
\end{python}
从variable\_def创建的一个变量对象。\newline
get\_shape():变量形状的别名。\newline
initialized\_value():返回初始化变量后的值。
\begin{python}
# Initialize 'v' with a random tensor.
v = tf.Variable(tf.truncated_normal([10, 40]))
# Use `initialized\_value` to guarantee that `v` has been
# initialized before its value is used to initialize `w`.
# The random values are picked only once.
w = tf.Variable(v.initialized_value() * 2.0)
\end{python}
\begin{python}
load(
    value,
    session=None
)
\end{python}
载入值进变量，写新的值进变量的存储区而不是增加操作到图上，这个方便的方法要求图上的所有变量在图被启动前绘画已经启动否则用默认的会话。
\begin{python}
v = tf.Variable([1, 2])
init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    # Usage passing the session explicitly.
    v.load([2, 3], sess)
    print(v.eval(sess)) # prints [2 3]
    # Usage with the default session.  The 'with' block
    # above makes 'sess' the default session.
    v.load([3, 4], sess)
    print(v.eval()) # prints [3 4]
\end{python}
ValueError: Session is not passed and no default session

read\_value():从当前上下文读入变量返回变量的值，可以不同于value（）如果他的值在另一个设备上，具有控制依赖等等。返回一个包含变量的Tensor。
\begin{python}
scatter_sub(
    sparse_delta,
    use_locking=False
)
\end{python}
从变量中减去IndexedSlices，parse\_delta:从这个变量减去的IndexedSlices，use\_locking:如果为True在操作过程中用locking。如果parse\_delta不是一个IndexedSlices将出现ValueError。
set\_shape(shape):重载变量的形状。\newline
to\_proto(export\_scope=None):转化变量为一个variableDef protocal buffer。export\_scope：字符串，移除的范围的名字，返回一个variableDef protocal buffer，如果Variable不在指定范围内为None。\newline
value():返回这些变量的快照，你不需要调用这个放放，当所有的操作需要变量的值时通过convert\_to\_tensor()自动调用它。返回一个保持变量值得Tensor，你不能指定一个新的值给这个tensor当它没有引用变量的时候。为了避免复制如果利用的返回值在同一个设备上作为变量，这实际上返回实时的值而不是复制的值。用过使用更新变量。如果使用在不同的设备上使用将得到不同的值，返回包含变量值的Tensor。
