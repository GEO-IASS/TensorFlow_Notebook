\section{layer}
\subsection{tf.layers.average\_pooling1d}
\textbf{函数功能}:一维数据的平均池化。

\begin{python}
average_pooling1d(
    inputs,
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None
)
\end{python}
参数:
\begin{itemize}
	\item 池化的Tensor，rank必须是3。
	\item pool\_size:一个正数或者是一个整数列表或元组，代表池化窗口大小。
	\item striders:一个整数，指定池化操作的步数。
	\item padding:一个字符串，padding的方法，可以是'valid'或者是'same'。
	\item data\_format:一个字符串channels\_last(默认)或者channels\_first,输入维度的顺序，channels\_last输入形状为(batch,length,channels)，channels\_first形状为(batch,channels,length)
	\item name:字符串，layer名字。
	\item 返回值:输出tensor，rank为3。
\end{itemize}
\subsection{tf.layers.average\_pooling2d}
\textbf{函数功能:}二维数据的平均池化
\begin{python}
average_pooling2d(
    inputs,
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None
)
\end{python}

参数:
\begin{itemize}
	\item inputs:池化的Tensor，rank必须为4
	\item pool\_size:两个元素的正数或者元组。指定池化窗口的大小。可以是单个整数表示，指定所有的空间维度为相同的值。
	\item padding:一个字符串，padding的方法，可以是'valid'或者是'same'
	\item data\_format:一个字符串channels\_last(默认)或者channels\_first,输入维度的顺序，channels\_last输入形状为(batch,height,width,channels)，channels\_first形状为(batch,channels,height,width)
	\item name:字符串，layer名字。
	\item 返回值:输出tensor。
\end{itemize}
\subsection{tf.layers.average\_pooling3d}
\textbf{函数功能:}三维输入的平均池化
\begin{python}
average_pooling3d(
    inputs,
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None
)
\end{python}
参数:
\begin{itemize}
	\item inputs:池化的Tensor，rank必须为5
	\item pool\_size:正数或者元组。列表（3个元素）指定池化窗口的大小。可以三个元素的整数，列表或者元组，指定所有的空间维度为相同的值。
	\item padding:一个字符串，padding的方法，可以是'valid'或者是'same'
	\item data\_format:一个字符串channels\_last(默认)或者channels\_first,输入维度的顺序，channels\_last输入形状为(batch,depth,height,width,channels)，channels\_first形状为(batch,depth,channels,height,width)
	\item name:字符串，layer名字。
	\item 返回值:输出tensor。
\end{itemize}
\subsection{tf.layers.batch\_normalization}
\textbf{函数功能:}batch normalization layer的函数接口。
\begin{python}
batch_normalization(
    inputs,
    axis=-1,
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer=tf.zeros_initializer(),
    gamma_initializer=tf.ones_initializer(),
    moving_mean_initializer=tf.zeros_initializer(),
    moving_variance_initializer=tf.ones_initializer(),
    beta_regularizer=None,
    gamma_regularizer=None,
    training=False,
    trainable=True,
    name=None,
    reuse=None,
    renorm=False,
    renorm_clipping=None,
    renorm_momentum=0.99,
    fused=False
)
\end{python}

batch normalization layer的函数\href{https://arxiv.org/pdf/1502.03167.pdf}{参考}，当训练的时候moving\_mean和moving\_variance需要被更新。默认的更新操作在tf.GraphKeys.UPDATE\_OPS,因此她们需要被添加到train\_op,例如
\begin{python}
  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
  with tf.control_dependencies(update_ops):
    train_op = optimizer.minimize(loss)
\end{python}
参数:
\begin{itemize}
	\item inputs:2维以上Tensor，进行normalizeation，第一个维度是batch\_size,如果data\_format是NHWC的最后一维data\_format是NCHW的第二维
	\item decay:滑动平均的衰退状态，接近于1，典型值是0.999,0.99,0.9。低的decay值是0.9，如果模型训练性能好但是在验证和测试性能差，尝试着设置zeros\_debias\_moving\_mean=True提高稳定性。
	\item center，bool型为Truebeta偏移到正则化的tensor，如果为False，beta被忽略。
	\item scale:bool型，如果为True，乘上gamma，如果为佳gamma不用。当下一层为线性(nn.relu)时scaling可能被下一层使用而禁用。
	\item epsilon:小的浮点数证驾到方差项防止除零错误。
	\item activation\_fn:激活函数，默认设置为None跳过激活函数使用线性激活函数。
	\item param\_initializers:beta,gamma,moving mean和moving variance优化选项
	\item param\_regularizers:beta和gamma的正则化选项。
	\item  updates\_collections:收集计算的更新操作。updates\_ops需要执行train\_op。如果为None，控制依赖将被增加保证更新在适当的位置计算。
	\item is\_training: 这层是否在训练模式如果在训练状态它将用指数移动平均求和加统计矩到moving\_mean和moving\_variance。当他不在在训练模式的时候她将用moving\_mean和moving\_variance的值。
	\item reuse: 是否层和它的变量被重用，重用层的范围必须给定。
	\item variables\_collections: 变量的选项收集。
	\item outputs\_collections:收集到增加输出。
	\item trainable: 如果为True增加变量到图GraphKeys.TRAINABLE\_VARIABLES上.
	\item batch\_weights:有batch\_size形状的tensor，包含每个批的频率，如果设置了值，批用权重均值和方差归一化（可以用来收集训练选中样本的偏置）。
	\item fused: 如果为True用一个更快的基于nn.fused\_batch\_norm的融合实现。如果为None，如果可能用fused实现。
	\item data\_format: 一个字符串，NHWC(默认)NVCHW也支持。
	\item zero\_debias\_moving\_mean:moving\_mean用一个zeros\_debias,它创建一个新的变量moving\_mean/biased
和'moving\_mean/local\\step'
	\item scope:变量范围的选项。
	\item renorm: 是否批\href{https://arxiv.org/pdf/1702.03275.pdf}{重新归一化},在训练过程中增加额外的变量，对于这个值得推倒时相同的
	\item renorm\_clipping: 用剪切重新归一化映射key'rmax','rmin','dmax'到标量Tensor的词典，相关的(r,d)被用作'corrected\_value = normalized\_value*r+d',r被剪切到[rmin.rmax],d到[-dmax,dmax],不指定rmax,rmin,dmax,dmin相应的被设置为inf和0.
	\item renorm\_decay:momentum在重新归一化中用于更新移动平均和标准差，太小（将增加噪声）太大（走样的评估）都将影响训练，decay用于在推理时得到均值和方差。
	\item 返回一个代表输出操作的Tensor。
	\item 异常
	\begin{itemize}
		\item ValueError:如果batch\_weights不是None但是fused是True时。
		\item ValueError:如果data\_format不是NHWC或者NCHW。
		\item ValueError:如果inputs的rank没有指定。
		\item ValueError:如果输入的channels或者rank没有指定。
	\end{itemize}
\end{itemize}
\subsection{conv1d}
\textbf{函数功能:}一维卷积层的函数接口，这个层创建一个卷积核和输出卷积输出一个tensor。如果use\_bias是True(bias\_initilizer被提供的话)，bias向量被创建添加到输出。最后如果激活函数不是None，激活函数也被用到输出。
\begin{python}
conv1d(
    inputs,
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format='channels_last',
    dilation_rate=1,
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    trainable=True,
    name=None,
    reuse=None
)
\end{python}

\begin{itemize}
	\item input:输入tensor。
	\item filters:整数,输出空间的维数（卷积核的数量）。
	\item kernel\_size:一个整数或者元组或者列表，指定一维卷积窗的大小。
	\item strides:一个单个整数，列表或者元组，指定stride的长度，指定任何不等于1的常数和指定任何dilation\_rate值部位1不兼容。
	\item padding:valid或者same。
	\item data\_format:一个字符串channels\_last(默认)或者channels\_first,输入维度的顺序，channels\_last输入形状为(batch,length,channels)，channels\_first形状为(batch,channels,length)
	\item dilation\_rate:一个整数或者元组或者列表，指定腐蚀卷积的腐蚀率，指定任何值dilation\_rate!=1和任何strides值!=1不兼容。
	\item activation:激活函数，设置None维持线性激活函数。
	\item use\_bias:bool型，是否层用bias。
	\item kernel\_initilizer:卷积核初始化器。
	\item bias\_initilizer:初始化偏置向量，如果为None将没有bias被用。
	\item activation\_regularizer:输出的正则化函数
	\item trainable:bool型，如果为True增加变量到GraphKeys.TRAINABLE\_VARIABLES
	\item name:layer的名字。
	\item reuse:是否重用之前层的相同名字的权重。
	\item 输出tensor。
\end{itemize}

\subsection{conv2d}
\textbf{函数功能:}这层创建一个卷积核和输入相卷积输出。如果use\_bias是True(bias\_initializer提供了)，bias向量被创建添加到输出，最后activation不是None，他被应用到输出。

\begin{python}
conv2d(
    inputs,
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format='channels_last',
    dilation_rate=(1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    trainable=True,
    name=None,
    reuse=None
)
\end{python}
\begin{itemize}
	\item input:输入tensor。
	\item filters:整数,输出空间的维数（卷积核的数量）。
	\item kernel\_size:2个整数元素或者元组或者列表，指定二维卷积窗的宽和高，可以一用一个整数指定所有空间维度相同。
	\item strides:一个整数，列表或者元组，指定stride的长度，指定任何不等于1的常数和指定任何dilation\_rate值不为1不兼容。
	\item padding:valid或者same。
	\item data\_format:一个字符串channels\_last(默认)或者channels\_first,输入维度的顺序，channels\_last输入形状为(batch,height,width,channels)，channels\_first形状为(batch,channels,height,width)
	\item dilation\_rate:两个整数或者元组或者列表，指定腐蚀卷积的腐蚀率，可以指定单个整数指定所有的空间维数相等，指定任何值dilation\_rate!=1和任何strides值!=1不兼容。
	\item activation:激活函数，设置None维持线性激活函数。
	\item use\_bias:bool型，是否层用bias。
	\item kernel\_initilizer:卷积核初始化器。
	\item bias\_initilizer:初始化偏置向量，如果为None将没有bias被用。
	\item activation\_regularizer:输出的正则化函数
	\item trainable:bool型，如果为True增加变量到GraphKeys.TRAINABLE\_VARIABLES
	\item name:layer的名字。
	\item reuse:是否重用之前层的相同名字的权重。
	\item 输出tensor。
\end{itemize}
\subsection{conv2d\_transpose}
\textbf{函数功能:}二维卷积层的接口函数，你希望用变形到正常卷积相反的方向你需要转置卷积，有时候一些卷积的输出形状和输入形状相同但是维持链接样式是兼容的。

\begin{python}
onv2d_transpose(
    inputs,
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format='channels_last',
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    trainable=True,
    name=None,
    reuse=None
)
\end{python}
\begin{itemize}
	\item input:输入tensor。
	\item filters:整数,输出空间的维数（卷积核的数量）。
	\item kernel\_size:2个正整数组成的元组或者列表，指定卷积核的宽和高，可以一用一个整数指定所有空间维度相同。
	\item strides:一个两个正整数组成的列表或者元组，指定stride的长度，指定任何不等于1的常数和指定任何dilation\_rate值不为1不兼容。
	\item padding:valid或者same。
	\item data\_format:一个字符串channels\_last(默认)或者channels\_first,输入维度的顺序，channels\_last输入形状为(batch,height,width,channels)，channels\_first形状为(batch,channels,height,width)
	\item dilation\_rate:两个整数或者元组或者列表，指定腐蚀卷积的腐蚀率，可以指定单个整数指定所有的空间维数相等，指定任何值dilation\_rate!=1和任何strides值!=1不兼容。
	\item activation:激活函数，设置None维持线性激活函数。
	\item use\_bias:bool型，是否层用bias。
	\item kernel\_initilizer:卷积核初始化器。
	\item bias\_initilizer:初始化偏置向量，如果为None将没有bias被用。
	\item activation\_regularizer:输出的正则化函数
	\item trainable:bool型，如果为True增加变量到GraphKeys.TRAINABLE\_VARIABLES
	\item name:layer的名字。
	\item reuse:是否重用之前层的相同名字的权重。
	\item 输出tensor。
\end{itemize}
\subsection{conv3d}
\textbf{函数功能:}三维卷积的函数接口。
这层创建一个卷积核和输入卷积生成输出tensor。如果use\_bias是True，bias\_initializer被提供，偏置向量创建添加到输出，最终如果激活函数不是None，激活函数被用在输出上。

\begin{python}
conv3d(
    inputs,
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    data_format='channels_last',
    dilation_rate=(1, 1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    trainable=True,
    name=None,
    reuse=None
)
\end{python}
\begin{itemize}
	\item input:输入tensor。
	\item filters:整数,输出空间的维数（卷积核的数量）。
	\item kernel\_size:3个正整数组成的元组或者列表，指定卷积核的深度，宽和高，可以一用一个整数指定所有空间维度相同。
	\item strides:三个正整数组成的列表或者元组，指定stride的深度，宽，高，指定单个整数代表所有空间维度相同，指定任何stride不等于1的常数和指定任何dilation\_rate值不为1不兼容。
	\item padding:valid或者same。
	\item data\_format:一个字符串channels\_last(默认)或者channels\_first,输入维度的顺序，channels\_last输入形状为(batch,depth,height,width,channels)，channels\_first形状为(batch,depth,channels,height,width)
	\item dilation\_rate:三个整数组成的元组或者列表，指定腐蚀卷积的腐蚀率，可以指定单个整数指定所有的空间维数相等，指定任何值dilation\_rate!=1和任何strides值!=1不兼容。
	\item activation:激活函数，设置None维持线性激活函数。
	\item use\_bias:bool型，是否层用bias。
	\item kernel\_initilizer:卷积核初始化器。
	\item bias\_initilizer:初始化偏置向量，如果为None将没有bias被用。
	\item activation\_regularizer:输出的正则化函数
	\item trainable:bool型，如果为True增加变量到GraphKeys.TRAINABLE\_VARIABLES
	\item name:layer的名字。
	\item reuse:是否重用之前层的相同名字的权重。
	\item 输出tensor。
\end{itemize}
\subsection{conv3d\_transpose}
\textbf{函数功能:}三维卷积函数接口

\begin{python}
conv3d_transpose(
    inputs,
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    data_format='channels_last',
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    trainable=True,
    name=None,
    reuse=None
)
\end{python}
\begin{itemize}
	\item input:输入tensor。
	\item filters:整数,输出空间的维数（卷积核的数量）。
	\item kernel\_size:3个正整数组成的元组或者列表,可以一用一个整数指定所有空间维度相同。
	\item strides:三个正整数组成的列表或者元组，指定单个整数代表所有空间维度相同。
	\item padding:valid或者same。
	\item data\_format:一个字符串channels\_last(默认)或者channels\_first,输入维度的顺序，channels\_last输入形状为(batch,depth,height,width,channels)，channels\_first形状为(batch,depth,channels,height,width)
	\item dilation\_rate:三个整数组成的元组或者列表，指定腐蚀卷积的腐蚀率，可以指定单个整数指定所有的空间维数相等，指定任何值dilation\_rate!=1和任何strides值!=1不兼容。
	\item activation:激活函数，设置None维持线性激活函数。
	\item use\_bias:bool型，是否层用bias。
	\item kernel\_initilizer:卷积核初始化器。
	\item bias\_initilizer:初始化偏置向量，如果为None将没有bias被用。
	\item activation\_regularizer:输出的正则化函数
	\item trainable:bool型，如果为True增加变量到GraphKeys.TRAINABLE\_VARIABLES
	\item name:layer的名字。
	\item reuse:是否重用之前层的相同名字的权重。
	\item 输出tensor。
\end{itemize}
\subsection{dense}
\textbf{函数功能:}这个层实现操作:output = activation(input.kernel+bias),这里activation传入activation的激活函数(如果不为None)，kernel是一个层创建的权重矩阵，bias是一个层创建的偏置向量。

\begin{python}
dense(
    inputs,
    units,
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    trainable=True,
    name=None,
    reuse=None
)
\end{python}
\begin{itemize}
	\item input:输入tensor。
	\item unit:整数或者长整数输出空间的维数。
	\item activation:激活函数，设置为None表示非线性函数。
	\item use\_bias:bool型，当前层是否使用bias
	\item kernel\_initializer:权重矩阵的初始化函数。
	\item bias\_initilizer:偏置的初始化函数。
	\item kernel\_relularizer:权重矩阵的正则化函数。
	\item bias\_regularizer:偏置的正则化函数。
	\item activation\_regularizer:输出的正则化函数。
	\item trainable:bool型，如果为True增加变量到GraphKeys.TRAINABLE\_VARIABLES
	\item name:layer的名字。
	\item reuse:是否重用之前层的相同名字的权重。
	\item 输出tensor。
\end{itemize}
\subsection{dropout}
\textbf{函数功能:}设置随机丢弃值得比率，帮助阻止过拟合。这个单位被$\frac{1}{1-rate}$,因此他们的和在训练和推理时不改变。

\begin{python}
dropout(
    inputs,
    rate=0.5,
    noise_shape=None,
    seed=None,
    training=False,
    name=None
)
\end{python}
\begin{itemize}
	\item input:输入tensor。
	\item dropout比率，值在0-1之间，例如rate=0.1表示丢掉输入的10\%。
	\item noise\_shape:int32类型的一维tensor代表二进制dropout mask乘上输入，例如，如果你的输入形状为(batch\_size, timesteps, features),你想dropout mask和所有的timesteps，你可以用noise\_shape=[batch\_size,1,features]
	\item seed:一个Python整数用于创建随机数种子。
	\item training:python bool或者TensorFlow bool标量tensor(例如placeholder)，是否在训练模式(dropout)或者在推理模式(返回没修改的输入)返回输出
	\item name:layer的名字。
	\item 输出tensor。
\end{itemize}
\subsection{max\_pool1d}
\textbf{函数功能:}一维输入的最大池化层。

\begin{python}
max_pooling1d(
    inputs,
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None
)
\end{python}
\begin{itemize}
\item inputs:需要池化的输入tensor，rank必须为3
\item pool\_size:一个整数或者列表或者元组，代表池化窗口的大小
\item strides:一个整数或者元组或者列表，指定池化操作的stride。
\item padding:一个字符串可以为'valid'或者'same'
\item data\_format:一个字符串，默认为channels\_last或者channels\_first.输入维度顺序，channels\_last相关输入的形状为(batch, length, channels)，channels\_first输出形状为(batch, channels, length)
\item name:一个字符串，layer的名字
\item 输出一个三维tensor。
\end{itemize}
\subsection{max\_pool2d}
\textbf{函数功能:}二维输入的最大池化

\begin{python}
max_pooling2d(
    inputs,
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None
)
\end{python}
\begin{itemize}
\item inputs:需要池化的输入tensor，rank必须为4
\item pool\_size:两个整数组成的元组或者列表(pool\_height,pool\_width)，代表池化窗口的大小,可以为单个整数表示所有空间维度相等。
\item strides:两个整数组成的元组或者列表(pool\_height,pool\_width)，指定池化操作的stride，可以为单个整数表示所有空间维度相等。
\item padding:一个字符串可以为'valid'或者'same'
\item data\_format:一个字符串，默认为channels\_last或者channels\_first.输入维度顺序，channels\_last相关输入的形状为(batch, height,width, channels)，channels\_first输出形状为(batch, channels, height,width)
\item name:一个字符串，layer的名字
\item 输出一个三维tensor。
\end{itemize}
\subsection{max\_pool3d}
\textbf{函数功能:}三维输入的最大池化层

\begin{python}
max_pooling3d(
    inputs,
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None
)
\end{python}
\begin{itemize}
\item inputs:需要池化的输入tensor，rank必须为5
\item pool\_size:三个整数组成的元组或者列表(pool\_depth,pool\_height,pool\_width)，代表池化窗口的大小,可以为单个整数表示所有空间维度相等。
\item strides:三个整数组成的元组或者列表(pool\_height,pool\_width)，指定池化操作的stride，可以为单个整数表示所有空间维度相等。
\item padding:一个字符串可以为'valid'或者'same'
\item data\_format:一个字符串，默认为channels\_last或者channels\_first.输入维度顺序，channels\_last相关输入的形状为(batch, depth,height,width, channels)，channels\_first输出形状为(batch, channels, depth,height,width)
\item name:一个字符串，layer的名字
\item 输出一个三维tensor。
\end{itemize}
\subsection{separable\_conv2d}
\textbf{函数功能:}深度方向分隔2维卷积层，这层执行深度方向上通过chennel分开的卷积接着在深度方向上混合通道。如果use\_bias是True并且bias初始化被提供了，它增加一个偏置向量到输出，选项应用激活函数生成最终输出。

\begin{itemize}
	\item inputs:需要池化的输入tensor
	item filters:整数,输出空间的维数（卷积核的数量）。
	\item kernel\_size:2个整数元素或者元组或者列表，指定二维卷积窗的宽和高，可以一用一个整数指定所有空间维度相同。
	\item strides:两个正整数组成的列表或者元组，可以一用一个整数指定所有空间维度相同，，指定任何不等于1的常数和指定任何dilation\_rate值不为1不兼容。
	\item padding:valid或者same。
	\item data\_format:一个字符串channels\_last(默认)或者channels\_first,输入维度的顺序，channels\_last输入形状为(batch,height,width,channels)，channels\_first形状为(batch,channels,height,width)
	\item dilation\_rate:两个整数或者元组或者列表，指定腐蚀卷积的腐蚀率，可以指定单个整数指定所有的空间维数相等，指定任何值dilation\_rate!=1和任何strides值!=1不兼容。
	\item depth\_multiplier:每个输入通道的深度方向卷积输出，总共的深度方向卷积数等于num\_filters\_in*depth\_multiplier
	\item activation:激活函数，设置None维持线性激活函数。
	\item use\_bias:bool型，是否层用bias。
	\item depthwise\_initializer:深度方向卷积核的初始化器
	\item pointwise\_initilizer:pointwise卷积核的初始化器。
	\item depthwise\_regularizer:depthwise卷积核的正则化器。
	\item pointwise\_regularizer:pointwise卷积核的正则化器。
	\item bias\_regularizer:偏置向量的正则化器。
	\item bias\_initilizer:初始化偏置向量，如果为None将没有bias被用。
	\item activation\_regularizer:输出的正则化函数
	\item trainable:bool型，如果为True增加变量到GraphKeys.TRAINABLE\_VARIABLES
	\item name:layer的名字。
	\item reuse:是否重用之前层的相同名字的权重。
	\item 输出tensor。

\end{itemize}