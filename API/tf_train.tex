\section{tf.train}
提供了训练模型的类和函数。
\subsection{优化器}
优化器类提供方法计算损失函数对于变量的的梯度的计算方法，子类集合实现了像A
dagrad和GradientDescent等经典算法。
\subsubsection{Optimizer}
基础的优化类，定义了增加一个操作到训练模型的API，你不直接需要这个类而是需要它的一些像GradientDescentOptimizer, AdagradOptimizer,或者MomentumOptimizer的子类。
\textbf{用法}\newline
\begin{python}
# Create an optimizer with the desired parameters.
opt = GradientDescentOptimizer(learning_rate=0.1)
# Add Ops to the graph to minimize a cost by updating a list of variables.
# "cost" is a Tensor, and the list of variables contains tf.Variable
# objects.
opt_op = opt.minimize(cost, var_list=<list of variables>)
\end{python}
在训练程序的过程中你需要返回操作。
\begin{python}
# Execute opt_op to do one step of training:
opt_op.run()
\end{python}
\textbf{在应用他们之前处理梯度}\newline
条用minimize()计算题度，应用它们在变量上。如果你想在应用他们之前处理你可以按照下面的步骤使用优化器。
\begin{enumerate}
	\item 用comput\_gradients()计算梯度。
	\item 按照你的希望处理梯度。
	\item 用apply\_gradients()处理梯度。
\end{enumerate}
\begin{python}
# Create an optimizer.
opt = GradientDescentOptimizer(learning_rate=0.1)

# Compute the gradients for a list of variables.
grads_and_vars = opt.compute_gradients(loss, <list of variables>)

# grads_and_vars is a list of tuples (gradient, variable).  Do whatever you
# need to the 'gradient' part, for example cap them, etc.
capped_grads_and_vars = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]

# Ask the optimizer to apply the capped gradients.
opt.apply_gradients(capped_grads_and_vars)
\end{python}
minimize（）he compute\_gradients()接受一个gate\_gradients参数控制fradient应用中的并行度。

GATE\_NONE:并行的计算，应用梯度，在执行过正中最大化并行程度，在结果中一些非重复性的代价。例如两个梯度的矩阵乘法依赖于输入值:GATE\_NONE可能被应用到输入前其他梯度被计算导致非重复性的结果。

GATE\_OP:对于每个Op，在他们使用之前确保所有的梯度被计算了。为了避免Op的race 为多个输入生成梯度condition，这里梯度依赖于输入。

GATE\_GRAPH:确保在它们任何一个被使用前所有变量的梯度被计算，提供了最小的并行化但是如果你想在应用他们之前处理所有的梯度这是很有用的。
\textbf{Slots}\newline
像MomentumOptimizer和AdagradOptimizer之类的优化器子类，结合变量训练分配管理额外的变量。这称为Slots，Slots有名字，你可以要求优化器它使用的名字。当你有一个slot名字你可以对变量要求优化器创建保留slot值。当你调试训练算法报告slots统计信息时很管用。
\textbf{方法}\newline
\textbf{\_\_init\_\_}
\begin{python}
__init__(
    use_locking,
    name
)
\end{python}
创建一个新的优化器，他必须通过子类的构造体调用。\newline
参数:
\begin{itemize}
	\item use\_locking:bool,如果为True用lock阻止当前变量更新。
	\item name:非空字符串为optimizer创建的累加器的名字。
	\item ValueError:名字格式不对。
\end{itemize}
\textbf{apply\_gradients}
\begin{python}
apply_gradients(
    grads_and_vars,
    global_step=None,
    name=None
)
\end{python}
应用梯度到变量尚，这是minimize()的第二部分，他返回应用梯度的Op。

参数:
\begin{itemize}
	\item grads\_add\_vars:返回compute\_gradients()的(梯度,变量)对列表。
	\item global\_step:变量被更新后此选项变量增加1.
	\item name:返回操作的名字。默认传递名字给优化器构造函数。
	\item [S] 返回:应用指定梯度的操作。如果global\_step不是None，操作增加global\_step
	\item 异常
	\begin{itemize}
		\item TypeError:如果grads\_and\_vars不对。
		\item ValueError:如果变量的梯度为none。
	\end{itemize}
\end{itemize}
\textbf{computer\_gradients}
\begin{python}
compute_gradients(
    loss,
    var_list=None,
    gate_gradients=GATE_OP,
    aggregation_method=None,
    colocate_gradients_with_ops=False,
    grad_loss=None
)
\end{python}
计算损失关于bar\_list的梯度，第一部分是minimize().返回一个(gradient,variable)对这里gradient是变量的梯度。注意梯度可以是tensor，IndexedSlices或者None（如果没有变量的梯度）
\begin{itemize}
	\item loss:包含需要最小化的值的tensor。
	\item var\_list:tf.Variable更新最小化loss的列表或者元组。默认图中的变量列表在GraphKey.TRAINABLE\_VARIABLES下。
	\item gate\_gradients:如何gate梯度计算，可以是GATE\_NONE,GATE\_OP或者是GATE\_GRAPH。
	\item aggregation\_method:指定结合梯度的方法，可用值定义在AggregationMethod。
	\item colocate\_gradients\_with\_ops:如果为True，尝试随着相关操作colocating梯度。
	\item grad\_loss:一个保持住loss梯度的Tensor。
	\item[S] 返回(gradient,variable)对，变量总是被呈现但是梯度可能是None。
\end{itemize}